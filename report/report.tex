\documentclass[12pt]{report}
\usepackage{url}
\usepackage{acro}
\usepackage{xcolor}
\usepackage{apacite}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{pgfgantt}
\usepackage{mathptmx} 
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{subcaption}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{ragged2e}
\usepackage[nottoc,notlof,notlot]{tocbibind} 
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[explicit]{titlesec}
\titleformat{\chapter}
[display]{\bfseries\centering}
{\huge Chapter \thechapter}
{1em}{\Huge #1}


\definecolor{tail}{RGB}{26, 188, 156}
\geometry{margin=1in}
\linespread{2}

\justifying
\setlength{\parindent}{2em}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\begin{document}
\begin{titlepage}
    \center
    \includegraphics[width = 15 cm]{./images/iium.png}
    \textsc{\LARGE }\\[1 cm]
    \textsc{\LARGE kulliyyah of information and}\\[.1 cm]
    \textsc{\LARGE communication technology}\\[1 cm]
    \textsc{\Large department of computer science}\\ [ .2 cm]
    \textsc{\large FINAL YEAR PROJECT REPORT}\\ [1.5 cm]
    \textsc{\Large sign language recognition using deep learning}\\ [1.5 cm]
    
    \textsc{\large mhd khaled maen}\\[.1 cm]
    \textsc{1523591}\\ [1 cm]
    
    
    \textsc{\large supervised by}\\[.1 cm]
    \textsc{\large assoc. prof. dr. amelia ritahani}\\[2 cm]
    
    \textsc{\large may} 2019\\ [.2 cm]
    \textsc{\large semester} 2, 2018 / 2019 \\
    
\end{titlepage}

\begin{titlepage}
    \center
    \textsc{\Huge FINAL YEAR PROJECT REPORT}\\ [1.5 cm]
  
    \textsc{\LARGE sign language recognition using deep learning}\\ [1 cm]
    
    {\large By}\\ [1 cm]
    
    \textsc{\large mhd khaled maen}\\[.1 cm]
    \textsc{1523591}\\ [1 cm]
    
    
    \textsc{\large supervised by}\\[.1 cm]
    \textsc{\large assoc. prof. dr. amelia ritahani}\\[1 cm]
    
    {In partial fulfillment of the requirement for the\par
    Bachelor of Computer Science\par
    Department of Computer Science\par
    Kulliyyah of Information and Communication Technology\par
    International Islamic University Malaysia
    }\\ [1.5 cm]

    \textsc{\large may} 2019\\ [.2 cm]
    \textsc{\large semester} 2, 2018 / 2019 \\
    
\end{titlepage}

\pagenumbering{roman}
% \maketitle
\begin{center}
    \LARGE DECLARATION
\end{center}   

I hear by declare that this report is the result of my own investigations,
except where otherwise stated. I also clear that it 
has not been previously or currently submitted as a whole for any other degree 
at IIUM or other institutions.

\mbox{}
\vfill
MHD KHALED MAEN (1523591)
\bigbreak
Signature: .................\hspace{18em} Date: .................
\bigbreak

\newpage
\begin{center}
    \huge {\textbf{APPROVAL PAGE}}    
\end{center} 

I certified that I have supervise can read this study and that in my opinion,
confirms to acceptable standards of scholarly presentation and is fully adequate,
in scope and quality, as final year project paper a partial fulfilment for a 
degree of bachelor of Computer Science (Honours).
\mbox{}
\vfill
Assoc. Prof. Dr. Amelia Ritahani (Supervisor)
\bigbreak
Department of Computer Science
\bigbreak
Kulliyyah of Information and Communication Technology
\bigbreak
International Islamic University Malaysia
\bigbreak

\newpage

\begin{center}
    \huge {\textbf{ABSTRACT}}
\end{center}

    Communication is an essential part of our life. Unfortunately, 
    some of us were born in various types of disability such as deaf, 
    since hearing impaired people cannot listen they loosed the ability 
    to learn how to speak so they developed a new communication way to 
    interact with other people by using distinct hand gestures, which was 
    not enough  to overcome this issue since most of hearing people do not 
    understand sign language, even now with all technologies and tools it 
    is remain a challenging problem to solve. For the mentioned reason, 
    the intention of this paper is to improve the ordinary model to translate
    the sign language gestures into a voice. In that, Deep learning is remarkably 
    serviceable for this mission, first by detecting a hand in a video frame using
    Convolutional Neural Network (CNN) algorithm followed by recognizing the 
    letter and state the matching sound. The accuracy achieved for a hand gesture
    detection using CNN model MobileNet-SSD is more than 90 \text{\%} for all the
    proposed signs.


\newpage
\begin{center}
    \huge {\textbf{ACKNOWLEDGEMENT}}
\end{center} 

This project has been completed with support from my supervisor,
Assoc. Prof. Dr. Amelia Ritahani, many thanks for her wonderful 
collaboration and consultation sessions. Furthermore, 
I would like to thank my coordinators, Asst. Prof. Dr. Hamwira Yaacob and 
Asst. Prof. Dr. Norzariyah Binti Yahya for their assistance helping me and 
others by organizing weekly sessions towards writing perfect report step-by-step. 
Last but not least, I want to thank my awesome parents for their support and time 
helping me reach the end of my undergraduate study, without them, I could not achieve that.


\setcounter{page}{2} 
\renewcommand{\contentsname}{TABLE OF CONTENTS}
\tableofcontents
\renewcommand*\listtablename{LIST OF TABLES} 
\listoftables
\renewcommand*\listfigurename{LIST OF FIGURES} 
\listoffigures
\newpage
\newpage
\begin{center}
    \huge{\textbf{LIST OF ABBREVIATIONS}}
\end{center} 

\textbf{FYP }   \tab(Final Year Project) \par
\textbf{CNN }   \tab(Convolutional Neural Network) \par 
\textbf{ASL }   \tab(American Sign Language) \par 
\textbf{DP}     \tab(Deep Learning) \par 
\textbf{ReLU}   \tab (Rectified Linear Unit) \par 
\textbf{FC}     \tab (Fully Connected Layer) \par 
\textbf{MC}     \tab (Multi Channel ) \par 
\textbf{GCP}    \tab (Google Cloud Platform) \par 
\textbf{GCE}    \tab (Google Compute Engine) \par 
\textbf{VM}     \tab (Virtual Machine) \par 
\textbf{Iaas}   \tab (Infrastructure as a Service) \par 
\textbf{CLI}    \tab (command Line interface) \par 
\textbf{API}    \tab (Application Programming Interface) \par
\textbf{GPU}    \tab (Graphics Processing Unit) \par
\textbf{CPU}    \tab (Central Processing Unit) \par
\textbf{XML}    \tab (eXtensible Mark-up Language) \par
\textbf{CSV}    \tab (Comma Separated Values) \par
\textbf{SSD}    \tab (Single Shot Detection) \par




\pagenumbering{arabic}
\chapter{Introduction} 

\section{Background}
Communication is a process of sending and receiving data among individuals. 
People communicate with each other's by a considerable measure of ways yet 
the best way is eye to eye correspondence. Numerous individuals trust that 
the significance of communication is like the importance of breathing. Indeed, 
communication facilitates the spread of knowledge and structures connections 
between individuals. 

Deep learning added an immense lift to the already rapidly developing field 
of computer vision. With deep learning, a lot of new utilization of computer
vision techniques have been presented and they are currently ending up some 
portion of our regular day to day existence.

Alongside with the intensity of the present computers, there are now various 
algorithms that were developed to empower the computers to perform tasks such 
as object tracking and pattern recognition. 

In this study, the attention will be on hand gestures detection and make an 
interpretation of them into voice.

\section{Problem Statement}
Communication difficulties arising from damage to hearing
directly have an effect on the standard of life. Difficulties in 
communication could end in deviations within the emotional and 
social development which will have a major impact on the standard of
lifetime of every one. It is well recognized that hearing is crucial 
to speech and language development, communication, and learning. People
with listening difficulties due to hearing loss or auditory processing 
problems continue to be an under-identified and under-served population.
The earlier the matter is known and intervention began, the less
serious the ultimate impact \cite{AFrajtag12017}.

The communication between hearing-impaired and other individuals is a 
colossal gab need to be filled up. In order to overcome this challenge 
many researches and products have been developed to solve this problem, 
but there is a lot to be enhanced.

\section{Objectives}
\begin{itemize}
    \item To collect sign language gestures dataset.
    \item To develop a hand gesture into voice algorithm.
    \item To construct a hand gesture into voice model.
    \item To evaluate and test the constructed model.
\end{itemize}

\section{Scope}
This research aims to develop a sign language recognition algorithm,
and converting it into voice.
\section{Significance}
Help the hearing-impaired community to communicate with hearing ones, 
in order to make a strong connected community.

\section{Timeline}
\begin{center}
    \begin{ganttchart}[
        expand chart=\textwidth,
        bar/.append style={draw=none, fill=tail},
        hgrid style/.style={draw=black!5, line width=.75pt},
        vgrid={*1{draw=black!5, line width=.75pt}},
        ]{1}{14}
        \gantttitle{Week}{14} \\
        \gantttitlelist{1,...,14}{1} \\
        \ganttbar{Title Selectin}{1}{3}  \\
        \ganttbar{Overall System Review}{2}{7}  \\
        \ganttbar{Literature Review}{4}{12}  \\
        \ganttbar{System Design}{8}{11}  \\
        \ganttbar{Design \& Prototype}{9}{12}  \\
        \ganttbar{Simulation}{7}{13}  \\
        \ganttbar{Report Writing}{5}{13}  \\
        \ganttbar{Submission}{13}{13}  \\
    \end{ganttchart}
\end{center}
\chapter{Literature review}

This chapter includes reviews of other previous researcher
and their proposed methods they used in implementing deep learning
to recognize hand gestures. These researches will help to grasp the knowledge
to achieve the project's objectives. 
\section{Sign Language}
In spoken languages words being pronounced by vocal cords which produce sound
wave can be heard, however for deaf people this sounds are not heard \cite{Ishizaka1972}, 
so sign language is based on the visual tools to communicate and receive 
information.
American Sign Language (ASL) is completely different from English. It has 
the basic features of a language such as the letters as shown in \ref{fig:sign_language}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./images/sign_language.png}
    \caption{Sign Language Alphabet. Retrieved from www.startASL.com}
    \label{fig:sign_language}
\end{figure}

\newpage
\section{Deep Learning}
Deep learning is a machine learning subfield that deals with algorithms based 
on the structure and function of the brain called artificial neural networks. 
In other words, it mirrors the brain's functioning.Deep learning algorithms 
are similar to the structure of the nervous system in which each neuron connects
and passes information. Deep learning models work in the layers and three layers 
of a typical model at least \ref{fig:deep_learining}. Each layer accepts and passes
the information from the previous layer to the next layer.

\begin{figure} [h]
    \centering
    \includegraphics[width=\textwidth]{./images/deep_learning.png}
    \caption{Typical deep learning model. Retrieved from www.medium.com}
    \label{fig:deep_learining}
\end{figure}

Deep learning models tend to perform well with quantity of data while 
old machine learning models do not improve after a saturation.

One of differences between machine learning and deep learning model 
is on the feature extraction area. Feature extraction is done by human
in machine learning whereas deep learning model figure out by itself.

\subsection{Activation Function}
Activation functions are functions that decide what the output of 
the node should be given the inputs into the node? 
Since the activation function determines the actual output, 
the outputs of a layer are often referred to as "activations."
The most well known activation functions are Sigmoid
\ref{fig:sigmoid}, Tanh \ref{fig:tanh} and
Rectified Linear Unit ReLU \ref{fig:relu}.

\subsubsection{Sigmoid Function:}
Non linear activation function with output in range (0,1).
$$ A = \frac{1}{1+e^{-x}} $$

\begin{figure} [h]
    \centering
    \includegraphics[width=.6\textwidth]{./images/sigmoid.png}
    \caption{Sigmoid Function}
    \label{fig:sigmoid}
\end{figure}

\subsubsection{Tanh Function:}
it is a scaled sigmoid function, bound to range (-1, 1).
$$ tanh(x) = \frac{2}{1+ e^{-2x}} - 1 $$

\begin{figure} [h]
    \centering
    \includegraphics[width=.5\textwidth]{./images/tanh.png}
    \caption{Tanh Function}
    \label{fig:tanh}
\end{figure}

\subsubsection{ReLU Function:}
less computationally expensive than tanh and sigmoid, it is range (-1, infinity).
$$ A(x) = max(0,x) $$

\begin{figure} [h]
    \centering
    \includegraphics[width=.5\textwidth]{./images/relu.jpeg}
    \caption{ReLU Function}
    \label{fig:relu}
\end{figure}

\subsection{Weights}
When input data enters a neuron, it is multiplied by a weight value 
which is assigned to the input. The neuron above the university example, 
for example, has two inputs, test scores and grades, so it has two related
weights that can be individually adjusted.
These weights start as random values and as the neural network learns more 
about what type of input data, the weights are adjusted based on any categorization
errors resulting from previous weights it can be associated as m in the original 
linear equation.
$$ y = mx + b$$


\subsection{Bias}
Same as weights, biases are the learnable parameters of the deep learning model.
The bias represents b in the original linear equation.
$$ y = mx + b $$

\section{Convolutional Neural Networks}
Convolutional Neural Networks are very similar to ordinary Neural Networks, 
they are made up of neurons that have learnable weights and biases.
The architectures of ConvNet make the explicit assumption that 
the inputs are images which will be encoded to certain properties in the architecture. 
This makes the forward function more efficient to implement and reduces the number of 
parameters in the network considerably.

Convolutional Neural networks allow computers to see, 
meaning that Convnets is used to recognize images by 
transforming the original image into a class scoring through layers.
CNN was inspired by the visual cortex.
Every time we see something, a series of layers of neurons gets activated, 
and each layer will detect a set of features such as lines, edges. 
The high level of layers will detect more complex features in order to
 recognize what we saw.

ConvNet has two parts: feature learning (Conv, Relu,and Pool) and 
classification(FC and softmax).

\subsection{Feature Learning}
\subsubsection{CONV layer:}
The objective of a Conv layer is to extract features of the input image.
A part of the image is connected to the next Conv layer because if all the 
input pixels are connected to the Conv layer, it is too expensive to compute. 
Therefore we will apply dot products in all dimensions between a receptive 
field and a filter. The result of this operation is a single volume integer 
(future map).
Then we slide the filter through a Stride over the next receiving field of 
the same input image and recalculate the dot products between the new receiving
field and the same filter. This process is repeated until we pass the entire 
input image. The output is the input on the next layer.

Some of the word that used interchangeably:
\begin{itemize}
    \item Filter (Kernel): a small matrix used to detect features.
    \item Feature Map: the output volume formed by sliding the filter 
    over the image and computing the dot product.
    \item Receptive field: a local region of the input image that has 
    the same size as the Kernel.
    \item Depth: the number of filters.
    \item Stride: has the objective of producing smaller output volumes spatially.
    \item Padding: the process of adding extra pixels over around the image.
\end{itemize}

\subsubsection{ReLU layer:}
Turning negative values into zeros. it has nothing to do with 
size of image and there are no hyperparameters.
\subsubsection{Pool Layer:}
Reduce the dimension of the input, the computational complexity of 
the model and controls the overfitting. There are different types of
pooling such as Average pooling, Max pooling and L2-norm pooling.
However, the Max pooling is the most use, which takes the most important
part (the pixel with the highest vule)\ref{fig:pool}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{./images/pool.png}
    \caption{Max pooling with 2x2 filter and stride = 2. Retrieved: Wikipedia}
    \label{fig:pool}
\end{figure}

\subsection{Classification}
\subsubsection{Fully Connected Layer(FC):}
Fully connected layers connect each neuron in a layer to every neuron 
in another layer. The last connected layer uses softmax activation function.
\subsubsection{Softmax:}
Activation function generate features input image into many 
classes based on the dataset.  

\bigbreak
\bigbreak


\begin{figure} [h]
    \centering
    \includegraphics[width=\textwidth]{./images/c_cnn.jpeg}
    
    \caption{Complete CNN architecture. Retrieved: medium.com}
    \label{fig:c_cnn}
\end{figure}



\section{Previous works}
Following the same path many previous attempts has been conducted 
to find a solution for this problem, marvelous work has been done, 
but most of them was just a prove of concept were the authors concentrate 
on a few signs to be detected.


\cite{Bao2017}, proposed a Deep convolutional neural network algorithm for hand-gesture 
recognition without hand localisation, since the hands only occupy about 10\% of 
the image. They used a combination of 9 convolution layers, 3 fully connected layers, 
interlaced with ReLU(Rectified Linear Unit) and dropout layers as shown in 
figure \ref{fig:tiny_architecture}. Alongside this architecture the apply some image 
processing techniques to have sufficient computation efficiency and memory requirement.
According to the paper the accuracy achieved was 97.1\% in the images with simple backgrounds
and 85.3\% in the images with complex backgrounds.However, the main disadvantage of of 
the proposed algorithm is the training set which only includes 7 different gestures,
and it tends to have bad accuracy with complex backgrounds.
\bigbreak

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{./images/tiny_b.png}
    \caption{Architecture of the proposed deep CNN }\label{fig:tiny_architecture}
\end{figure}

\clearpage

\cite{Rao2018}, proposed a CNN architecture for classifying selfie sign 
language gestures. 
The CNN architecture is designed with four convolutional layers. 
Each convolutional layer with different filtering window sizes 
as shown in figure \ref{fig:selfie}  
They had a dataset with five different subjects performing 200 
signs in 5 different viewing angles under various background environments.
Each sign occupied for 60 frames or images in a video.
The proposed model performed training on 3 batches to 
test the robustness of different training mode 
using caffe deep learning framework. However, the result accuracy 
was 92.88\% need more training and improvements. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{./images/selfie.png}
    \caption{Proposed Deep CNN architecture}
    \label{fig:selfie}
\end{figure}




\cite{Hussain2017}, introduced a CNN based classifier  trained through 
the process of transfer learning
over a pretrained convolutional neural network which is trained on a 
large dataset. We are using VGG16 figure \ref{fig:vgg16} as the pretrained model.
The According to the paper the accuracy was 93.09\%,while using AlexNet 
figure \ref{fig:alexnet} was 76.96\%. the same problem here with the other papers 
which is the small number of sign that begin trained on 7 signs, and the accuracy
need to be improved as well.
\bigbreak

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{images/vgg16.png}
    \caption{VGG16 architecture. Retrieved from www.cs.toronto.edu}
    \label{fig:vgg16}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{images/alexnet.png}
    \caption{AlexNet architecture. Retrieved from www.saagie.com}
    \label{fig:alexnet}
\end{figure}


\clearpage

\cite{Pyo2016}, introduced a depth-based hand data with convolution
neural networks (CNNs).The hand gesture dataset has roughly 6,000 RGB-D 
images in each of 12 labels. In all, there are approximately 60,000
training images, 1S,000 validation images, and 12,000 training images.
each time they were increasing the number of layers and testing the 
accuracy \ref{fig:depth_cnn}.
They came with result that more number of layers, does not guarantee 
the increase of accuracy.
\bigbreak

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/depth_cnn.png}
    \caption{Classification results of modified networks}
    \label{fig:depth_cnn}
\end{figure}

\bigbreak
\cite{Devineau2018}, introduced a 3D hand gesture recognition 
approach based on a deep learning model using Convolutional 
Neural Network (CNN). The proposed model only uses hand-skeletal 
data and no depth image.
The model produced by  multi-channel convolutional neural network
with two feature extraction modules and a residual
branch per channel \ref{fig:m_cnn}.
The achieved accuracy was a 91.28\text{\%} 
classification accuracy for the 14 gesture
classes case and an 84.35\text{\%} classification 
accuracy for the 28 gesture classes case.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/m_cnn.png}
    \caption{parallel convolutional neural network}
    \label{fig:m_cnn}
\end{figure}


\clearpage
\section{Summary}

This chapter illustrated some works have been done previously on
hand gesture and sign language recognition using Machine Vision and
deep learning (Convolutional Neural Network).
Table \ref{table:summary} shows the summary of the literature review.

\begin{center}
    \begin{table}[h]
        \caption{Summary of the literature review}
        \begin{tabular}{ |p{7cm}|p{2cm}|p{2cm}|p{3cm}| }
            \hline
            Title & Year & Accuracy & Software\\
            \hline
            Tiny Hand Gesture Recognition without Localization via a 
            Deep Convolutional Network & 2017 & 97.1\%& CNN \\
            \hline
            Deep Convolutional Neural Networks for Sign Language 
            Recognition & 2018 & 92.88\% & CNN \\
            \hline
            Hand Gesture Recognition Using Deep Learning & 2017 & 93.09\% & CNN VGG16 \\
            \hline
            Depth-based Hand Gesture Recognition using 
            Convolutional Neural Networks & 2016 & 95.57\% & CNN \\
            \hline
            Deep Learning for Hand Gesture Recognition on Skeletal 
            Data & 2018 & 91.28\% & MC-DCNN \\
            \hline
        \end{tabular}
        \label{table:summary}
    \end{table}
\end{center}
\newpage

\chapter{Methodology}

Image recognition, voice producing, 
system design block diagram figure \ref{fig:system_diagram} 
and the flowchart of the research is presented in details 
alongside with the tools and algorithms in this chapter.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{./images/system_diagram.png}
    \caption{System block diagram}
    \label{fig:system_diagram}
\end{figure}


\clearpage
In order to get a model with satisfying results, 
we setup an environment with a powerful computational 
resources and large dataset.

\section{Environment}
Google Cloud Platform (GCP) is a cloud computing service 
provided by Google which provides a lot of products including 
cloud computing.
Google Compute Engine (GCE) is the Infrastructure as a Service (IaaS) 
enables users to setup Virtual Machines (VMs)  can be accessed via 
the developer console, command line interface (CLI) and RESTful API \cite{google_cloud}.
The VM instance of choice  run on an Ubuntu operating system version 
16.04. Alongside, the VM instance is powered by 8 Graphics Processing Units 
(GPUs) of type NVIDIA Tesla K80  which engineered to boost throughput in 
real-world applications by 5-10x times, compared to Central Processing Unit (CPU).

\section{Data set}
The data set is a collection of images for American Sign Language (ASL) alphabet,
contained of 87,000 images of 200x200 pixels frame size as shown in Figure 3. 
There are 29 classes, the first 26 classes are the letters A to Z and the last
three classes are space delete and empty signs \cite{kaggle_dataset}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./images/letter_k.jpg}
    \caption{The letter K in American Sign language}
    \label{fig:letter_k}
\end{figure}

\newpage
\section{Image processing}
In order to extract useful information from an image, 
some methods need to be implemented:
\subsection{Image annotation}
The process of annotating and labeling every object in 
the image in this case the sign has been labeled, 
so the machine can identify the important part of the 
image which has the object and the characteristics of it.
The data of annotated image saved in eXtensible Mark-up 
Language (XML) file such as object class and the bounding 
box coordination Figure \ref{fig:annotation}.
\newline
\begin{figure} [h]
    \centering
    \includegraphics[width=.6\textwidth]{./images/annotation.png}
    \caption{XML file for letter A}
    \label{fig:annotation}
\end{figure}

\newpage
\subsection{XML to Comma Separated Values (CSV)}
The goal of this process is to convert the semi-structured data 
stored in the XML files into structured data stored in one file 
CSV so it can be transferred into TensorFlow(TF)Records Figure \ref{fig:csv}.
\newline
\begin{figure} [h]
    \centering
    \includegraphics[width=.8\textwidth]{./images/csv.png}
    \caption{CSV file for image test data}
    \label{fig:csv}
\end{figure}

\section{TensorFlow Object Detection API}
Open source framework built on top of TensorFlow in order to help 
researchers to construct, train and deploy object detection models \cite{tf_od_api}. 

\section{algorithm}
The algorithm used in this experiment is MobileNet-SSD as shown 
in Figure \ref{fig:mobilenetssd}; MobileNet is a base network which use depth-wise 
separable convolutions to build light weight deep neural networks 
provide high level features for classification or detection, 
after removing the fully connected layer at the end of the network 
and replace it with Single Shot Multibox Detector (SSD) one of 
the most popular algorithm in object detections \cite{Liu2016}. 
SSD generates a box with score of presence of each object category 
and adjusts the box to better match the object shape. In addition SSD 
eliminates proposal generation and feature resampling stage by 
encapsulates all computation in a single network, alongside with 
the ability to combines predictions from multiple features maps 
with different resolutions to naturally handle objects of various sizes.

\begin{figure} [h]
    \centering
    \includegraphics[width=\textwidth]{./images/mobilenetssdh.png}
    \caption{MobileNet-SSD  Retrieved from https://hey-yahei.cn/2018/08/08/MobileNets-SSD/}
    \label{fig:mobilenetssd}
\end{figure}


\section{Hand detection}
The problem of hand recognition that hand occupied usually less than 25 percent of the image.
To overcome this issue the model should be provided with high accurate detection algorithm,
Right now there are so many good algorithms for object detection which can be utilize to 
detect a human hand.

\subsection{Single-Shot Detector (SSD)}

Unlike Faster R-CNN which perform regional proposals 
and region classifications in two steps. SSD does the two in a "single shot"
jointly predict the bounding box and the class while it processes the image.

how it's work?
\begin{itemize}
    \item Generate a set of feature maps with different scales by passing 
    the image through sequence of convolutional layers (10x10, 6x6, 3x3 ...).
    \item Use a 3*3 convolutional filter to evaluate bounding boxes for each 
    location of the feature maps.
    \item predict bounding box of set and the class probability all together.
    \item The best predicted box called as "positive" label, alongside with
    the boxes that have IoU \footnote{Intersection over Union } value $>$ 0.5 
\end{itemize}
Sense SSD skip filtering step, it generates multiple bounding box with multiple shapes
and most of them are negative example.

To fix this issue, SSD does two extra methods.
First, non-maximum suppression:\footnote{Object detection methods often output 
multiple detections which fully or partly cover the same object in an image.}
to group overlapping boxes into one box by keeping the highest confidence
Then,hard negative mining: to balance classes during the training process; 
subset the negative examples with the highest training loss with a 3:1 ratio 
of negatives for positives.\cite{Liu2016}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{./images/ssd.png}
    \caption{SSD. Retrieved from https://www.semanticscholar.org}
    \label{fig:frcnn}
\end{figure} 


\newpage
\section{Voice producing}
After processing the image the CNN algorithm classify the gesture
that presented in the image, the corresponding text (word, char, number)
will be generated as voice that Simulate the human voice. 


\chapter{Results and analysis}
According to the results obtained after training the deep learning model for 13587 steps,
the model being able to detect and identify each and every sign of the 29 signs including 
the empty one with classification loss 0.143 and localization loss 0.0327
as shown in figure \ref{fig:d_eval} and figure \ref{fig:loss}
\bigbreak

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{./images/d_eval.png}
    \caption{Prediction of letter D}
    \label{fig:d_eval}
\end{figure} 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./images/loss.png}
    \caption{Loss graphs}
    \label{fig:loss}
\end{figure}

\chapter{Conclusion and Future Work}
The ability of Communication is the base Right to each individual in society,
and it's our responsibility to help and interact with hearing-impaired people,
so they can feel that they are an important part in the world.
This model improves the interaction between the hearing-impaired people 
and the healthy people by recognition the hand gestures of American Sign Language (ASL)
then pronouncing it as sound in real time. Future work will focus on 
further improvement of the deep learning model by increasing the training time 
and the used dataset size and quality, alongside with adding gestures for numbers 
and complete words.

\chapter*{APPENDIX I}
\addcontentsline{toc}{chapter}{APPENDIX I}  
The entire project including the code, report and technical report is published 
on github for open-source community.
\linebreak
    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{./images/github.png}
        \caption{github repository}
        \label{fig:github}
    \end{figure}
\renewcommand\bibname{References}            
\bibliography{scope.bib}
\bibliographystyle{apacite}
\end{document}